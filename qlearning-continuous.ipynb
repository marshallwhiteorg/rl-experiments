{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.stats.entropy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c465c34b59ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.stats.entropy'"
     ]
    }
   ],
   "source": [
    "'''Implements Q-learning for a continuous state space using a feed-forward neural network to approximate Q'''\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import logging\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from lib import plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats.entropy as entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "#env = gym.wrappers.Monitor(env, 'tmp/training-dir/')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    ''' Simple feed-forward NN for approximating the Q-function\n",
    "    1 hidden layer'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, observation, *, num_actions, epsilon):\n",
    "    '''Select an action using the e-greedy policy based on the model\n",
    "\n",
    "    :param model: torch.nn model\n",
    "    :param observation: observation from the environment to select the action for\n",
    "    :param num_actions: Size of action space\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :returns: an action from [0, num_actions)\n",
    "    :rtype: Int\n",
    "    '''\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        return model(torch.Tensor(observation)).argmax().item()\n",
    "\n",
    "\n",
    "def q_learning(env, model, optimizer, *, num_episodes, alpha, gamma, epsilon, max_entropy):\n",
    "    \"\"\"Run Q-learning (with neural network function approximator)\n",
    "\n",
    "    :param env: OpenAI environment\n",
    "    :param model: torch.nn model\n",
    "    :param optimizer: torch.optim optimizer\n",
    "    :param num_episodes: Number of episodes to run\n",
    "    :param alpha: Learning rate\n",
    "    :param gamma: Discount factor\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :param max_entropy: Boolean indicating inclusion/exclusion of entropy bonus\n",
    "    :returns: Statistics\n",
    "    :rtype: plotting.EpisodeStats\n",
    "\n",
    "    \"\"\"\n",
    "    statistics = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    nA = env.action_space.n\n",
    "    q = defaultdict(lambda: np.zeros(nA))\n",
    "    for episode_idx in range(num_episodes):\n",
    "        if (episode_idx + 1) % 10 == 0:\n",
    "            print(\"\\nEpisode {}/{}\"\n",
    "                  .format(episode_idx + 1, num_episodes))\n",
    "        observation = env.reset()\n",
    "        terminal = False\n",
    "        t = 1\n",
    "        while not terminal:\n",
    "            action = select_action(model, observation, num_actions=nA, epsilon=epsilon)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            statistics.episode_rewards[episode_idx] += reward\n",
    "            statistics.episode_lengths[episode_idx] = t\n",
    "            prediction_distribution = model(torch.Tensor(observation))\n",
    "            prediction = prediction_distribution[action]\n",
    "            if max_entropy:\n",
    "                entropy_bonus = entropy(prediction_distribution.data)\n",
    "                # not sure if bonus should go in target or prediction\n",
    "                target = reward + gamma * model(torch.Tensor(next_observation)).max() + entropy_bonus\n",
    "            else:\n",
    "                target = reward + gamma * model(torch.Tensor(next_observation)).max()\n",
    "            loss = nn.functional.mse_loss(prediction, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if done:\n",
    "                terminal = True\n",
    "            else:\n",
    "                observation = next_observation\n",
    "                t += 1\n",
    "    return statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeedForwardNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d72dfa2059b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForwardNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_RUNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeedForwardNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 1\n",
    "NUM_EPISODES = 100\n",
    "INPUT_DIM = 4\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "model = FeedForwardNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "for run_idx in tqdm(range(NUM_RUNS)):\n",
    "    run_stats = q_learning(env, model, optimizer, num_episodes=NUM_EPISODES, alpha=.1, gamma=1, epsilon=.05, max_entropy=False)\n",
    "    plotting.plot_episode_stats(run_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "qlearning-continuous.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
