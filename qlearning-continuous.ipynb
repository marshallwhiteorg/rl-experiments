{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''Implements Q-learning for a continuous state space (cartpole)'''\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import logging\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from lib import plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "#env = gym.wrappers.Monitor(env, 'tmp/training-dir/')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    ''' Simple feed-forward NN for approximating the Q-function\n",
    "    1 hidden layer'''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.reLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc2(self.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, observation, *, num_actions, epsilon):\n",
    "    '''Select an action using the e-greedy policy based on the model\n",
    "\n",
    "    :param model: model to approximate Q\n",
    "    :param observation: observation from the environment to select the action for\n",
    "    :param num_actions: Size of action space\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :returns: an action from [0, num_actions)\n",
    "    :rtype: Int\n",
    "    '''\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        return model(torch.Tensor(observation)).argmax()\n",
    "\n",
    "\n",
    "def q_learning(env, *, num_episodes, alpha, gamma, epsilon, max_entropy):\n",
    "    \"\"\"Find the optimal policy using off-policy Q-learning\n",
    "\n",
    "    :param env: OpenAI environment\n",
    "    :param num_episodes: Number of episodes to run\n",
    "    :param alpha: Learning rate\n",
    "    :param gamma: Discount factor\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :returns: Optimal Q-function and statistics\n",
    "    :rtype: dictionary of state -> action -> action-value, plotting.EpisodeStats\n",
    "\n",
    "    \"\"\"\n",
    "    statistics = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    nA = env.action_space.n\n",
    "    q = defaultdict(lambda: np.zeros(nA))\n",
    "    for episode_idx in range(num_episodes):\n",
    "        if (episode_idx + 1) % 10 == 0:\n",
    "            print(\"\\nEpisode {}/{}\"\n",
    "                  .format(episode_idx + 1, num_episodes))\n",
    "        observation = env.reset()\n",
    "        terminal = False\n",
    "        t = 1\n",
    "        while not terminal:\n",
    "            policy = make_policy(q, env.action_space.n, epsilon)\n",
    "            action_distribution = policy(observation)\n",
    "            action = np.random.choice(np.arange(len(action_distribution)),\n",
    "                                      p=action_distribution)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            #print(observation, reward, action, next_observation, done)\n",
    "            next_observation = torch.tensor(next_observation)\n",
    "            statistics.episode_rewards[episode_idx] += reward\n",
    "            statistics.episode_lengths[episode_idx] = t\n",
    "            next_action_values = [q[next_observation][next_action]\n",
    "                                  for next_action\n",
    "                                  in np.arange(nA)]\n",
    "            best_next_q = max(q[next_observation])\n",
    "            entropy_bonus = entropy(action_distribution)\n",
    "            if max_entropy:\n",
    "                q[observation][action] += alpha * (reward + gamma * best_next_q - q[observation][action] + entropy_bonus)\n",
    "            else:\n",
    "                q[observation][action] += alpha * (reward + gamma * best_next_q - q[observation][action])\n",
    "            if done:\n",
    "                terminal = True\n",
    "            else:\n",
    "                observation = next_observation\n",
    "                t += 1\n",
    "    return q, statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "num_episodes = 100\n",
    "for run_idx in tqdm(range(num_runs)):\n",
    "    q, stats = q_learning(env, num_episodes=num_episodes, alpha=.1, gamma=.99, epsilon=.1, max_entropy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "qlearning-continuous.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
