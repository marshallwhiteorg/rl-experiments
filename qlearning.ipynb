{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''Implements Q-learning'''\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import logging\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from lib import plotting\n",
    "#from utils.format import get_obss_preprocessor\n",
    "import torch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "ENV_NAME = 'Blackjack-v0'\n",
    "#ENV_NAME = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#ENV_NAME = 'MiniGrid-Empty-5x5-v0'\n",
    "#ENV_NAME = 'CartPole-v0'\n",
    "#ENV_NAME = 'Taxi-v2'\n",
    "env = gym.make(ENV_NAME)\n",
    "#env = gym_minigrid.wrappers.FlatObsWrapper(gym.make(ENV_NAME))\n",
    "logging.info(ENV_NAME)\n",
    "# A function that will format the observation space for easier use\n",
    "#_, preprocess_obss = get_obss_preprocessor(env.observation_space)\n",
    "\n",
    "\n",
    "def make_policy(q, num_actions, epsilon):\n",
    "    \"\"\"Make an epsilon-greedy policy from the provided Q-function\n",
    "\n",
    "    :param q: Q-function\n",
    "    :param num_actions: Size of action space\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :returns: Stochastic policy as a function of the observation\n",
    "    :rtype: fn: observation -> [action probability]\n",
    "\n",
    "    \"\"\"\n",
    "    def policy(observation):\n",
    "        best_action_idx = np.argmax(q[observation] + 1e-10 * np.random.random(q[observation].shape))\n",
    "        distribution = []\n",
    "        for action_idx in range(num_actions):\n",
    "            probability = epsilon / num_actions\n",
    "            if action_idx == best_action_idx:\n",
    "                probability += 1 - epsilon\n",
    "            distribution.append(probability)\n",
    "        return distribution\n",
    "    return policy\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes, alpha, gamma, epsilon, *, max_entropy):\n",
    "    \"\"\"Find the optimal policy using off-policy Q-learning\n",
    "\n",
    "    :param env: OpenAI environment\n",
    "    :param num_episodes: Number of episodes to run\n",
    "    :param alpha: Learning rate\n",
    "    :param gamma: Discount factor\n",
    "    :param epsilon: Probability of taking a random action\n",
    "    :returns: Optimal Q-function and statistics\n",
    "    :rtype: dictionary of state -> action -> action-value, plotting.EpisodeStats\n",
    "\n",
    "    \"\"\"\n",
    "    statistics = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    nA = env.action_space.n\n",
    "    q = defaultdict(lambda: np.zeros(nA))\n",
    "    for episode_idx in range(num_episodes):\n",
    "        if (episode_idx + 1) % 10 == 0:\n",
    "            print(\"\\nEpisode {}/{}\"\n",
    "                  .format(episode_idx + 1, num_episodes))\n",
    "        observation = torch.tensor(env.reset())\n",
    "        terminal = False\n",
    "        t = 1\n",
    "        while not terminal:\n",
    "            policy = make_policy(q, env.action_space.n, epsilon)\n",
    "            action_distribution = policy(observation)\n",
    "            action = np.random.choice(np.arange(len(action_distribution)),\n",
    "                                      p=action_distribution)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            #print(observation, reward, action, next_observation, done)\n",
    "            next_observation = torch.tensor(next_observation)\n",
    "            statistics.episode_rewards[episode_idx] += reward\n",
    "            statistics.episode_lengths[episode_idx] = t\n",
    "            next_action_values = [q[next_observation][next_action]\n",
    "                                  for next_action\n",
    "                                  in np.arange(nA)]\n",
    "            best_next_q = max(q[next_observation])\n",
    "            entropy_bonus = entropy(action_distribution)\n",
    "            if max_entropy:\n",
    "                q[observation][action] += alpha * (reward + gamma * best_next_q - q[observation][action] + entropy_bonus)\n",
    "            else:\n",
    "                q[observation][action] += alpha * (reward + gamma * best_next_q - q[observation][action])\n",
    "            if done:\n",
    "                terminal = True\n",
    "            else:\n",
    "                observation = next_observation\n",
    "                t += 1\n",
    "    return q, statistics\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q, stats = q_learning(env, 5000, .1, .99, .1, max_entropy=False)\n",
    "    plotting.plot_episode_stats(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
